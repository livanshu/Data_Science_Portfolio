{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIQOOG+2wuORRiliPj3qJH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ytZ0yuIt9tw"
      },
      "source": [
        "# Data Cleaning:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "2jy5zFO-uAl1",
        "outputId": "a98acd1d-75aa-4c0c-98d2-afcbc22d0d2f"
      },
      "source": [
        "## Overviewing the dataset ##\r\n",
        "\r\n",
        "#top 5 rows\r\n",
        "data.head()\r\n",
        "\r\n",
        "\r\n",
        "## MISSING VALUES ##\r\n",
        "\r\n",
        "# count\r\n",
        "missing_values_count = data.isnull().sum()\r\n",
        "\r\n",
        "# look at the # of missing points in the first ten columns\r\n",
        "missing_values_count[0:10]\r\n",
        "\r\n",
        "# missing percentage\r\n",
        "total_cells = np.product(data.shape)\r\n",
        "total_missing = missing_values_count.sum()\r\n",
        "percent_missing = (total_missing/total_cells) * 100\r\n",
        "\r\n",
        "# Drop missing values\r\n",
        "data.dropna() #only rows\r\n",
        "data.dropna(axis=1) #only columns\r\n",
        "\r\n",
        "# Replace missing values\r\n",
        "data.fillna(0) #replace with zeros\r\n",
        "data.fillna(method='bfill', axis=0).fillna(0) #replace with value that comes directly after it in the same column, zero to remaning\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## SCALING & NORMALIZATION ##\r\n",
        "\r\n",
        "# mix-max scale the data between 0 and 1\r\n",
        "from mlxtend.preprocessing import minmax_scaling\r\n",
        "data = minmax_scaling(original_data, columns=[0])\r\n",
        "\r\n",
        "# plot with minmax and without minmax scaling\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "original = np.random.exponential(size=100)\r\n",
        "scaled = minmax_scaling(original_data, columns=[0])\r\n",
        "fig, ax = plt.subplots(1,2)\r\n",
        "sns.distplot(original_data, ax=ax[0])\r\n",
        "ax[0].set_title(\"Original\")\r\n",
        "sns.distplot(scaled_data, ax=ax[1])\r\n",
        "ax[1].set_title(\"Scaled\")\r\n",
        "\r\n",
        "# normalize with boxcox\r\n",
        "from scipy import stats\r\n",
        "normalized_data = stats.boxcox(data)\r\n",
        "\r\n",
        "# plot with and without normalization\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "fig, ax=plt.subplots(1,2)\r\n",
        "sns.distplot(original_data, ax=ax[0])\r\n",
        "ax[0].set_title(\"Original Data\")\r\n",
        "sns.distplot(normalized_data[0], ax=ax[1])\r\n",
        "ax[1].set_title(\"Normalized data\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## DATE PARSING ##\r\n",
        "\r\n",
        "# convert date columns to datetime\r\n",
        "import datetime\r\n",
        "data['date_parsed'] = pd.to_datetime(data['date'], format=\"%m/%d/%y\")\r\n",
        "\r\n",
        "# Some examples:\r\n",
        "# 1/17/07 has the format \"%m/%d/%y\"\r\n",
        "# 17-1-2007 has the format \"%d-%m-%Y\"\r\n",
        "\r\n",
        "# to get day of the month\r\n",
        "day_of_month = data['date_parsed'].dt.day\r\n",
        "day_of_month.head()\r\n",
        "\r\n",
        "# plot day of the month\r\n",
        "sns.distplot(day_of_month, kde=False, bins=31)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## CHARACTER ENCODING ##\r\n",
        "\r\n",
        "#UTF-8 is the standard text encoding. \r\n",
        "#All Python code is in UTF-8 and, ideally, all your data should be as well. \r\n",
        "#It's when things aren't in UTF-8 that you run into trouble.\r\n",
        "\r\n",
        "# check to see what datatype it is\r\n",
        "type(data)\r\n",
        "\r\n",
        "# testing\r\n",
        "before = \"This is the euro symbol: â‚¬\"\r\n",
        "type(before)\r\n",
        "#OUTPUT: str\r\n",
        "after = before.encode(\"utf-8\", errors=\"replace\")\r\n",
        "type(after)\r\n",
        "#OUTPUT: byte\r\n",
        "\r\n",
        "after = before.encode(\"ascii\", errors=\"replace\")\r\n",
        "print(after.decode(\"ascii\"))\r\n",
        "\r\n",
        "# to check encoding type\r\n",
        "with open(\"../path\", 'rb') as rawdata:\r\n",
        "    char_encoding = chardet.detect(rawdata.read(10000))\r\n",
        "print(char_encoding)\r\n",
        "\r\n",
        "# reading file using result encoding type\r\n",
        "data = pd.read_csv(\"../path.csv\", encoding='Windows-1252') # Windows-1252 came from checking encoding\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## INCONSISTENT DATA ENTRY ##\r\n",
        "\r\n",
        "# Fuzzy matching #\r\n",
        "# get the top 10 closest matches to \"lolo\"\r\n",
        "data = fuzzywuzzy.process.extract(\"lolo\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\r\n",
        "# take a look at them\r\n",
        "data\r\n",
        "\r\n",
        "# function to replace rows in the provided column of the provided dataframe\r\n",
        "# ratios to be taken from fuzzy results i.e. data\r\n",
        "def replace_matches_in_column(df, column, string_to_match, min_ratio = 50):\r\n",
        "    # get a list of unique strings\r\n",
        "    strings = df[column].unique()\r\n",
        "    \r\n",
        "    # get the top 10 closest matches to our input string\r\n",
        "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \r\n",
        "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\r\n",
        "\r\n",
        "    # only get matches with a ratio > 90\r\n",
        "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\r\n",
        "\r\n",
        "     # get the rows of all the close matches in our dataframe\r\n",
        "    rows_with_matches = df[column].isin(close_matches)\r\n",
        "\r\n",
        "    # replace all rows with close matches with the input matches \r\n",
        "    df.loc[rows_with_matches, column] = string_to_match\r\n",
        "    \r\n",
        "    # let us know the function's done\r\n",
        "    print(\"Done\")\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-62c266a35f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#top 5 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    }
  ]
}