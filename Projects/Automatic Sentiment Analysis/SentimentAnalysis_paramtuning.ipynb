{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis_paramtuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIzePlIOQMHj4Eug9Dvu0X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/livanshu/Data_Science_Portfolio/blob/main/Projects/Automatic%20Sentiment%20Analysis/SentimentAnalysis_paramtuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM1ofDo1KNrE"
      },
      "source": [
        "# Parameter Tuning \n",
        "\n",
        "One way to tune your hyperparameters for any given Machine Learning algorithm is to perform a grid search over all the possible combinations of values. If your hyperparameters can be any real number, you will need to limit the search to some finite set of possible values for each hyperparameter. For efficiency reasons, often you might want to tune one individual parameter, keeping all others constant, and then move onto the next one; Compared to a full grid search there are many fewer possible combinations to check, and this is what you'll be doing for the questions below.\n",
        "\n",
        "Try the following values for T: [1, 5, 10, 15, 25, 50] and the following values for λ [0.001, 0.01, 0.1, 1, 10]. For pegasos algorithm, first fix λ=0.01 to tune T, and then use the best T to tune λ.\n",
        "\n",
        "Following code is a part of sentiment_analysis/main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfQpARyuKIPG"
      },
      "source": [
        "data = (train_bow_features, train_labels, val_bow_features, val_labels)\n",
        "\n",
        "# values of T and lambda to try\n",
        "Ts = [1, 5, 10, 15, 25, 50]\n",
        "Ls = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "print(\"Perceptron\")\n",
        "opt_T, opt_accu = tune_pct(*data, Ts)\n",
        "print(opt_T, opt_accu)\n",
        "\n",
        "print(\"Average Perceptron\")\n",
        "opt_T, opt_accu = tune_pct(*data, Ts, avg_pct=True)\n",
        "print(opt_T, opt_accu)\n",
        "\n",
        "# fix values for L and T while tuning Pegasos T and L, respective\n",
        "print('Pegasos: tune T')\n",
        "fix_L = 0.01\n",
        "(fix_T, opt_L), opt_accu = tune_peg(*data, Ts, [fix_L])\n",
        "print((fix_T, opt_L), opt_accu)\n",
        "\n",
        "print('Pegasos: tune L')\n",
        "(opt_T, opt_L), opt_accu = tune_peg(*data, [fix_T], Ls)\n",
        "print((opt_T, opt_L), opt_accu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD9OhXmuKnNY"
      },
      "source": [
        "### Performance after tuning:\n",
        "\n",
        "Perceptron:\n",
        "T = 25, \n",
        "val acc = 0.794\n",
        "\n",
        "\n",
        "Average Perceptron:\n",
        "T = 25,\n",
        "val acc = 0.8\n",
        "\n",
        "\n",
        "Pegasos- tune T:\n",
        "T = 25,\n",
        "λ = 0.01,\n",
        "val acc = 0.806\n",
        "\n",
        "\n",
        "Pegasos- tune L:\n",
        "T = 25,\n",
        "λ = 0.01,\n",
        "val acc = 0.806"
      ]
    }
  ]
}